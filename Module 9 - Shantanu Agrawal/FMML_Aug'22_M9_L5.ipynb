{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUfWQF7flq1d"
      },
      "source": [
        "# Foundation of Modern Machine Learning\n",
        "## Module 9: Neural Networks\n",
        "## Lab 5: MLP for regression\n",
        "#### Module Coordinator: Shantanu Agrawal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xGNv5-orKuU"
      },
      "source": [
        "You must be thinking that MLP are better used for classification purposes. But after going through the Tensorflow Playground, you would have seen that MLP can be used for Regression problems as well.\n",
        "\n",
        "Also, we had seen regression problems in an earlier lab. In this lab, we will see how to do the MLP implementation for these kinds of problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rVmEwGYnS69"
      },
      "source": [
        "# Demonstration on simple datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUx4HRtQ5vvi"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Q_uQcgeYD6"
      },
      "source": [
        "# MLP for Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a simple synthetic dataset and attempt to perform regression."
      ],
      "metadata": {
        "id": "3oxeM5G-XLC4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvTLgfjkrAo5"
      },
      "source": [
        "torch.manual_seed(1)    \n",
        "\n",
        "# We generate data with a simple function\n",
        "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  \n",
        "y = x.pow(2) + 0.2*torch.rand(x.size())                 \n",
        "\n",
        "x, y = Variable(x), Variable(y)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n",
        "plt.title('Regression Analysis')\n",
        "plt.xlabel('Independent varible')\n",
        "plt.ylabel('Dependent varible')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBZcMXPzxjL4"
      },
      "source": [
        "def train(x,y,model,epochs,optimizer,loss_func):\n",
        "  \"\"\" Function for training \"\"\"\n",
        "  losses = []\n",
        "\n",
        "  for t in range(epochs):\n",
        "    \n",
        "    prediction = model(x)     \n",
        "\n",
        "    loss = loss_func(prediction, y)     \n",
        "    losses.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()   \n",
        "    loss.backward()         \n",
        "    optimizer.step()        \n",
        "  \n",
        "  return losses, prediction\n",
        "\n",
        "def plot_training(x,y,prediction,losses,num_epochs):\n",
        "\n",
        "  epochs = np.arange(num_epochs)\n",
        "\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  plt.scatter(x.data.numpy(),y.data.numpy(),color=\"orange\")\n",
        "  plt.plot(x.data.numpy(),prediction.data.numpy(),color=\"green\")\n",
        "  plt.show()\n",
        "\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  plt.title(\"Loss vs Epochs\")\n",
        "  plt.plot(epochs,losses,color=\"red\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Final loss: {}\".format(losses[-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a simple MLP and train it on our synthetic dataset."
      ],
      "metadata": {
        "id": "xUNrSmlzXTuX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGKWEblnrI_J"
      },
      "source": [
        "# Simple MLP using PyTorch\n",
        "net1 = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1,10),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(10,1)\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.SGD(net1.parameters(), lr=0.2)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "losses1,prediction1 = train(x,y,net1,200,optimizer,loss_func)\n",
        "plot_training(x,y,prediction1,losses1,200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try other architectures as well."
      ],
      "metadata": {
        "id": "ig_HEomYXaxe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XP2LP_MzkbD"
      },
      "source": [
        "# Another architecture\n",
        "net2 =  torch.nn.Sequential(\n",
        "        torch.nn.Linear(1, 200),\n",
        "        torch.nn.LeakyReLU(),\n",
        "        torch.nn.Linear(200, 100),\n",
        "        torch.nn.LeakyReLU(),\n",
        "        torch.nn.Linear(100, 1),\n",
        "    )\n",
        "\n",
        "optimizer = torch.optim.Adam(net2.parameters(), lr=0.05)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "losses2,prediction2 = train(x,y,net2,2000,optimizer,loss_func)\n",
        "plot_training(x,y,prediction2,losses2,2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try training our model with some other synthetic datasets."
      ],
      "metadata": {
        "id": "z24FVj0WXgVW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YXJ7LqThPcq"
      },
      "source": [
        "# Sine wave\n",
        "x = torch.unsqueeze(torch.linspace(-10, 10, 1000), dim=1)  \n",
        "y = torch.sin(x) + 0.2*torch.rand(x.size())                 \n",
        "\n",
        "x, y = Variable(x), Variable(y)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n",
        "plt.title('Sine wave')\n",
        "plt.xlabel('Independent varible')\n",
        "plt.ylabel('Dependent varible')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhvXfVYphS6b"
      },
      "source": [
        "net3 = torch.nn.Sequential(\n",
        "        torch.nn.Linear(1, 200),\n",
        "        torch.nn.LeakyReLU(),\n",
        "        torch.nn.Linear(200, 100),\n",
        "        torch.nn.LeakyReLU(),\n",
        "        torch.nn.Linear(100, 1),\n",
        "    )\n",
        "\n",
        "optimizer = torch.optim.Adam(net3.parameters(), lr=0.05)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "losses3,prediction3 = train(x,y,net3,2000,optimizer,loss_func)\n",
        "plot_training(x,y,prediction3,losses3,2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuiJX2qso0Y6"
      },
      "source": [
        "# Using a real world dataset ( Boston housing prices dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KbBFZ9fneg3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from os import path\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#scikit-learn related imports\n",
        "import sklearn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# pytorch relates imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ-jCgzVneg6"
      },
      "source": [
        "## Data loading and pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UE4Jl0eneg6"
      },
      "source": [
        "Let's load boston house prices dataset and corresponding labels from scikit-learn library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jy2NwbTneg7"
      },
      "source": [
        "boston = load_boston()\n",
        "\n",
        "# feature_names -> ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "feature_names = boston.feature_names\n",
        "\n",
        "X = boston.data\n",
        "y = boston.target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyqqZS_neg7"
      },
      "source": [
        "In order to retain deterministic results, let's fix the seeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWzpN-nSneg8"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ3b2O_Dneg9"
      },
      "source": [
        "Let's use 70% of our data for training and the remaining 30% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6F1gniEneg-"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhbzehj0nehB"
      },
      "source": [
        "# Tensorizing inputs and creating batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFv5JbB_nehC"
      },
      "source": [
        "Below we tensorize input features and corresponding labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP9Qi0wJnehC"
      },
      "source": [
        "X_train = torch.tensor(X_train).float()\n",
        "y_train = torch.tensor(y_train).view(-1, 1).float()\n",
        "\n",
        "X_test = torch.tensor(X_test).float()\n",
        "y_test = torch.tensor(y_test).view(-1, 1).float()\n",
        "\n",
        "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_iter = torch.utils.data.DataLoader(datasets, batch_size=10, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGK-YqjnehC"
      },
      "source": [
        "Defining default hyper parameters for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yGxAQ42nehC"
      },
      "source": [
        "batch_size = 50\n",
        "num_epochs = 200\n",
        "learning_rate = 0.0001\n",
        "size_hidden1 = 100\n",
        "size_hidden2 = 50\n",
        "size_hidden3 = 10\n",
        "size_hidden4 = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8f7oL-dnehD"
      },
      "source": [
        "We define a four layer neural network containing ReLUs between each linear layer. This network is more complex than the standard linear regression model and results in a better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3kRCTy1nehD"
      },
      "source": [
        "class BostonModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(13, size_hidden1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.lin4 = nn.Linear(size_hidden3, size_hidden4)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.lin4(self.relu3(self.lin3(self.relu2(self.lin2(self.relu1(self.lin1(input)))))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qke1IPFLnehD"
      },
      "source": [
        "model = BostonModel()\n",
        "model.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBhypKDJnehE"
      },
      "source": [
        "## Train Boston Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyt-sjCznehE"
      },
      "source": [
        "Defining the loss function that will be used for optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps8RlgiYnehE"
      },
      "source": [
        "criterion = nn.MSELoss(reduction='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQF3f1qonehF"
      },
      "source": [
        "Defining the training function that contains the training loop and uses RMSprop and given input hyper-parameters to train the model defined in the cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQuEwyvunehF"
      },
      "source": [
        "def train(model_inp, num_epochs = num_epochs):\n",
        "    optimizer = torch.optim.RMSprop(model_inp.parameters(), lr=learning_rate)\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_iter:\n",
        "            # forward pass\n",
        "            outputs = model_inp(inputs)\n",
        "            # defining loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # computing gradients\n",
        "            loss.backward()\n",
        "            # accumulating running loss\n",
        "            running_loss += loss.item()\n",
        "            # updated weights based on computed gradients\n",
        "            optimizer.step()\n",
        "        if epoch % 20 == 0:    \n",
        "            print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
        "                  (epoch + 1, num_epochs, running_loss))\n",
        "        running_loss = 0.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6O_BQMepPtB"
      },
      "source": [
        "train(model, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05IhVzI2orGH"
      },
      "source": [
        "# Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S2EjSp2nehH"
      },
      "source": [
        "model.eval()\n",
        "outputs = model(X_test)\n",
        "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
        "\n",
        "print('Model error: ', err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further experiments\n",
        "\n",
        "1. Try experimenting with the architecture of the model. What kind of results can you obtain?\n",
        "  - Try to explain why the particular change in the architecture brings the following change in the result.\n",
        "2. Try using a different dataset suitable for regression and training on this dataset. Can you compare performance with a simple linear or polynomial regression based model?\n",
        "  - Datasets are available in *sklearn.datasets* library as well.\n",
        "\n",
        "# Checking your progress\n",
        "\n",
        "1. Till this lab, you should be able to write up the code for the basic MLP model?\n",
        "2. You should be able to understand the basic components for implementing the basic MLP model architecture?\n",
        "  - like loss functions, optimizers, training and testing functions, model class generations, etc.\n",
        "  - how will be the flow of the data in the model (as given in **forward()** function).\n",
        "3. You should also look for the frequently used ways to implement the above mentioned componenets?"
      ],
      "metadata": {
        "id": "dixP73_FXsx8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRVq2wEJYONF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}